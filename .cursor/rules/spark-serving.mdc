<!--
title: Spark Serving – SGLang/vLLM/Ollama (operational rules)
last_update: 2026-01-21
status: draft
scope:
  summary: "Regeln für LLM-Serving Entscheidungen und Debugging im Spark-Setup."
  in_scope:
    - endpoint correctness (base URL, /v1 paths, model IDs)
    - SGLang flags that impact client compatibility
    - safe advice for Cursor/OpenWebUI integration
  out_of_scope:
    - deep GPU tuning
    - public exposure / security hardening beyond basics
notes: []
-->

## Client-Kompatibilität (Cursor/OpenWebUI)
- Docker-Hostnames wie `sglang-qwen-uncensored` funktionieren **nicht** als Client-URL von Windows aus.
- Verwende immer Spark-Tailscale-IP/MagicDNS + Port.

## OpenAI-kompatible APIs
- Wenn ein Client Modell-Liste nutzt: stelle sicher, dass `/v1/models` **einen sauberen Modellnamen** liefert.
- Für SGLang: nutze `--served-model-name <name>` statt dass `/model` als ID auftaucht.
- Teste mit `/health`, `/v1/models`, `/v1/chat/completions` bevor du “Cursor-Konfig kaputt-toggelst”.

## Cursor: Keine globalen Overrides
- Vermeide “Override OpenAI Base URL” dauerhaft; das kann Cursor-eigene Modelle (GPT‑5.2 etc.) brechen.

## Cursor UI Footgun: bestehende Chats cachen Provider/Model
- Nach Änderungen an Base URL / API Key / Model ggf. **neuen Chat öffnen** oder **Reload Window**.
- Sonst kann Cursor “We’re having trouble connecting…” zeigen, obwohl der Endpoint per `curl /v1/models` OK ist.

## Funnel (Internet): niemals “Dummy-Key” als Security
- Wenn Tailscale **Funnel** genutzt wird, ist der Endpoint internet-exponiert.
- **`sk-local` o. ä. ist KEINE Security** (leicht erratbar). Für Funnel immer einen **starken Bearer Token** erzwingen
  (am Proxy/Caddy) und den Token **nur lokal auf Spark** speichern (nicht committen).

## Quantization Reality (GB10 / SM121): “läuft” vs “schnell”
- **FP8 ist unser Default**, wenn es “einfach laufen” soll (gute Performance, breite Unterstützung).
- **NVFP4/MXFP4 (FP4)** ist das Ziel-Format (Blackwell), aber Performance hängt stark vom Engine‑Build/Kernels ab (Fallbacks möglich).
- **`compressed-tensors` vermeiden (Stand heute)**: viele Community‑FP8/NVFP4 Repos sind `compressed-tensors` (float‑quantized) und können in SGLang an fehlenden Schemes/Loadern scheitern.
  - Quick Check: `config.json` → `quantization_config.quant_method`. Bei `compressed-tensors`: lieber anderen Checkpoint wählen oder Engine/Image upgraden.

## Kernel/Backend Footguns (praktisch)
- Wenn FP4/NVFP4 Modelle “komisch” performen oder Backends zicken: für SGLang auf GB10 ist `--attention-backend triton` oft der robusteste Pfad.

